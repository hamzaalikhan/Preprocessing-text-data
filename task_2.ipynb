{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used:\n",
    "* bs4 for using beautiful soup in order to parse files.\n",
    "* re for regular expressions\n",
    "* os to join file path\n",
    "* nltk.tokenize for tokenizing raw text\n",
    "* collections to count word frequency\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This project comprises of 2 tasks; this jupyter notebook contains task 2. There are 139 raw text files. Our task here is to generate spare representation for the raw text, create a vocablary for the sparse files and create segment boundaries for the sparse text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Defining file path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the file paths\n",
    "stopWords = \"./stopwords_en.txt\"\n",
    "theText = \"./txt_files/\"\n",
    "segFile = './topic_segs.txt'\n",
    "vocab = './vocab.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1  Function to create a set of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating set of stop words\n",
    "#opening the file\n",
    "s = open(stopWords,'r')\n",
    "\n",
    "#reading the file line by line\n",
    "message = s.readlines()\n",
    "\n",
    "#list to store all the stop words\n",
    "sW=[]\n",
    "#using loop to create the list\n",
    "for items in message:\n",
    "    items1 = items.strip()\n",
    "    sW.append(items1)\n",
    "\n",
    "#Creating the set of stopped words\n",
    "stopwords_set = set(sW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2  Functions to create segment boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create topic_segs vectors\n",
    "def topicSegs1(file,fileType,stopwordSet):\n",
    "    \n",
    "    #Creating the name of he file without .txt\n",
    "    fileName = file[:-4]\n",
    "    stars=['**********']\n",
    "    #joining paths\n",
    "    file_text = os.path.join(fileType, file)\n",
    "    \n",
    "    #reading each line in text file\n",
    "    f = open(file_text,'r')\n",
    "    \n",
    "    #creating list with removed newline chracters\n",
    "    x=[]\n",
    "    for lines in f:\n",
    "        if lines!='**********\\n':\n",
    "            lines=lines.lower()\n",
    "            tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "            unigram_tokens = tokenizer.tokenize(lines)\n",
    "            final_tokens = [w for w in unigram_tokens if w not in stopwordSet]\n",
    "            if len(final_tokens)>0:\n",
    "                x1=[]\n",
    "                for items in final_tokens:\n",
    "                    if len(items)>2:\n",
    "                        x1.append(items)\n",
    "                x.append(x1)\n",
    "            \n",
    "        else:\n",
    "            x.append(lines.split())\n",
    "    \n",
    "    x.append(stars)\n",
    "    x1=[]\n",
    "    for items in x:\n",
    "        if len(items)>0:\n",
    "            x1.append(items)\n",
    "    return x1\n",
    "\n",
    "#Using the list from the topicsegs1 function to create vector bolean\n",
    "def topicSegs2(file,fileType,stopwords):\n",
    "    fileName = file[:-4]\n",
    "    x= topicSegs1(file,fileType,stopwords)\n",
    "    t=[]\n",
    "    for items in x:\n",
    "        if items!=['**********']:\n",
    "            t.append(str(0))\n",
    "        else:\n",
    "            if len(t)>0:\n",
    "                t[-1]=str(1)\n",
    "    t1=\",\".join(t)\n",
    "    t2= fileName+':'+t1\n",
    "    return t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Functions that help create the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to tokenize and remove stop words\n",
    "def parsingText(file,fileType,stopwords_set):\n",
    "    \n",
    "    #joining file path\n",
    "    file_text = os.path.join(fileType, file)\n",
    "    f = open(file_text,'r')\n",
    "    \n",
    "    #reading the whole file and converting it into a lower case string\n",
    "    message = f.read().lower()\n",
    "    \n",
    "    #tokenzing the string using the regex provided\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "    unigram_tokens = tokenizer.tokenize(message)\n",
    "    \n",
    "    #creating a list of the tokens\n",
    "    setTokens=list(set(unigram_tokens))\n",
    "    \n",
    "    #removing the stop words\n",
    "    finalTokens = [w for w in setTokens if w not in stopwords_set]\n",
    "    \n",
    "    #returning the file list of tokens\n",
    "    finalTokens1=[]\n",
    "    for items in finalTokens:\n",
    "        if len(items)>2:\n",
    "            finalTokens1.append(items)\n",
    "    return finalTokens1\n",
    "\n",
    "#Empty list to store the list of words and the topic segments\n",
    "allWords = []\n",
    "\n",
    "\n",
    "#Running loop over all the files\n",
    "for file in os.listdir(theText):\n",
    "    if file!= '.DS_Store':\n",
    "    #Calling function to get list of words and adding it to another list\n",
    "        fileWords=parsingText(file,theText,stopwords_set)\n",
    "        allWords.extend(fileWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4  Final function to create the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using counter function to check frequency of word occurance\n",
    "counts = Counter(allWords)\n",
    "\n",
    "#loop that adds word who's frequency is more than 132 to list of stopwords\n",
    "for items in counts:\n",
    "    \n",
    "    #Checking if values is greater than 132\n",
    "    if counts[items]>132:\n",
    "        stopwords_set.add(items)\n",
    "\n",
    "#Removing extra words including stop words from the final List of words\n",
    "final_tokens = [w for w in allWords if w not in stopwords_set]\n",
    "\n",
    "#creating a set of sorted vocab words\n",
    "final_tokens= set(final_tokens)\n",
    "finalVocab = sorted(final_tokens)\n",
    "\n",
    "#empty dictionary to create dictionary of words\n",
    "vocabDict={}\n",
    "i=0\n",
    "\n",
    "#Adding vocab key values to the dictionary of vocab words\n",
    "while i < len(finalVocab):\n",
    "    vocabDict[finalVocab[i]]=i\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening vocab file to read in the whole vocab\n",
    "f = open(vocab,'w')\n",
    "\n",
    "#writing values from dictionary to the text file\n",
    "for items in vocabDict:\n",
    "    vocabIndex=str(items)+':'+str(vocabDict[items])+'\\n'\n",
    "    \n",
    "    #adding vocab to the text file\n",
    "    f.write(vocabIndex)\n",
    "    \n",
    "#Closing the file once it is done\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5  Function to create sparse representation of the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparseText(file,fileType):\n",
    "\n",
    "    file_text = os.path.join(fileType, file)\n",
    "    f=open(file_text,'r')\n",
    "    finaLX=[]\n",
    "    i=0\n",
    "    for lines in f:\n",
    "        x=[]\n",
    "        for words in lines.split():\n",
    "            words=words.lower()\n",
    "            tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "            tword = tokenizer.tokenize(words)\n",
    "            \n",
    "            for word in tword:\n",
    "                \n",
    "                if word in vocabDict:\n",
    "                    x.append(vocabDict[word])\n",
    "                    \n",
    "        i=i+1\n",
    "        if len(x)>0:\n",
    "            counts = Counter(x)\n",
    "            finaLX.append(counts)\n",
    "    return finaLX\n",
    "\n",
    "def finalSparse(file, fileType):\n",
    "    x= sparseText(file,fileType)\n",
    "    finalList=[]\n",
    "    for items in x:\n",
    "        theList=[]\n",
    "        x1=''\n",
    "        for key,values in items.items():\n",
    "            x= str(key)+':'+str(values)\n",
    "            x1+=x+','\n",
    "            #print(x)\n",
    "            x3=x1[:-1]\n",
    "        theList.append(x3)\n",
    "        finalList.extend(theList)\n",
    "    x= '\\n'.join(finalList)\n",
    "    return x\n",
    "        \n",
    "\n",
    "\n",
    "for file in os.listdir(theText):\n",
    "    if file!=\".DS_Store\":\n",
    "        finalFile = './sparse_files/'+file[:-4]+'.txt'\n",
    "        f = open(finalFile,'w')\n",
    "        f.write(finalSparse(file,theText))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSegs = []\n",
    "#Running loop over all the files\n",
    "for file in os.listdir(theText):\n",
    "    if file!=\".DS_Store\":\n",
    "        fileSegs=topicSegs2(file,theText,stopwords_set)\n",
    "        allSegs.append(fileSegs)\n",
    "#creating the file for the topic segments\n",
    "finalSegs = \"\\n\".join(allSegs)\n",
    "#opening the file\n",
    "f = open(segFile,'w')\n",
    "#adding output to the file\n",
    "f.write(finalSegs)\n",
    "\n",
    "#closing the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "1. In this project task, the input files were from raw text files created in task 1.\n",
    "2. Several different approaches were used to create the sparse representation of the raw text.\n",
    "3. The summary of the task is as follows:\n",
    "    - Created a list that have the vector bolean for the segments of the topic.\n",
    "    - Created dictionary to store the vocablary.\n",
    "    - Created sparse representation of raw text after tokenzing it and matching corresponding words to the vocab\n",
    "4. The final outputs were exported to relavant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
